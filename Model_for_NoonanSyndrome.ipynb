{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0140577-c9c3-41a7-b9b2-4c4e4c994ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147efddd-0772-4d2d-97ed-4a5c46940559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding \n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512  # Optional: controls how large each text chunk is for embedding\n",
    "\n",
    "df1 = pd.read_csv('llmdataset.csv')\n",
    "\n",
    "documents = [Document(text=' '.join(row.values.astype(str))) for _, row in df1.iterrows()]\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "print(\"Vector index created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c003bc-c092-4a61-a625-a0c323353d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "system_prompt = (\"You are a Q&A assistant. Your goal is to answer questions as accurately as possible based on the context provided to you.\")\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e407117d-3e9c-4c61-bda8-79b7a7b58219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d1a5ce-a0ce-4d6a-a695-781d31c67c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "stopping_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  \n",
    "]\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    context_window=8192,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=stopping_ids,\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.chunk_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3eb2a6-2f82-4e96-8a16-f356e211df46",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1f66e5-a920-493c-ad7a-35300250b466",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()\n",
    "def predict(input, history=None):\n",
    "    response = query_engine.query(input)\n",
    "    return str(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879926df-914b-4170-8d79-fa0eb7f1b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = input(\"Ask a question about Noonan Syndrome: \")\n",
    "response = query_engine.query(user_question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e21509-7fb4-4fa4-ad8d-f3d426427650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "gr.ChatInterface(predict).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
